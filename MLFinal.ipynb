{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.optimizers import Adam\n",
    "import pandas as pd, numpy as np, os, gc\n",
    "import math\n",
    "from keras import callbacks\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000000 rows of TRAIN.CSV!\n",
      "Only using 1500000 rows to train and validate\n"
     ]
    }
   ],
   "source": [
    "# LOAD AND FREQUENCY-ENCODE\n",
    "FE = ['EngineVersion','AppVersion','AvSigVersion','Census_OSVersion']\n",
    "# LOAD AND ONE-HOT-ENCODE\n",
    "OHE = [ 'RtpStateBitfield','IsSxsPassiveMode','DefaultBrowsersIdentifier',\n",
    "        'AVProductStatesIdentifier','AVProductsInstalled', 'AVProductsEnabled',\n",
    "        'CountryIdentifier', 'CityIdentifier', \n",
    "        'GeoNameIdentifier', 'LocaleEnglishNameIdentifier',\n",
    "        'Processor', 'OsBuild', 'OsSuite',\n",
    "        'SmartScreen','Census_MDC2FormFactor',\n",
    "        'Census_OEMNameIdentifier', \n",
    "        'Census_ProcessorCoreCount',\n",
    "        'Census_ProcessorModelIdentifier', \n",
    "        'Census_PrimaryDiskTotalCapacity', 'Census_PrimaryDiskTypeName',\n",
    "        'Census_HasOpticalDiskDrive',\n",
    "        'Census_TotalPhysicalRAM', 'Census_ChassisTypeName',\n",
    "        'Census_InternalPrimaryDiagonalDisplaySizeInInches',\n",
    "        'Census_InternalPrimaryDisplayResolutionHorizontal',\n",
    "        'Census_InternalPrimaryDisplayResolutionVertical',\n",
    "        'Census_PowerPlatformRoleName', 'Census_InternalBatteryType',\n",
    "        'Census_InternalBatteryNumberOfCharges',\n",
    "        'Census_OSEdition', 'Census_OSInstallLanguageIdentifier',\n",
    "        'Census_GenuineStateName','Census_ActivationChannel',\n",
    "        'Census_FirmwareManufacturerIdentifier',\n",
    "        'Census_IsTouchEnabled', 'Census_IsPenCapable',\n",
    "        'Census_IsAlwaysOnAlwaysConnectedCapable', 'Wdft_IsGamer',\n",
    "        'Wdft_RegionIdentifier']\n",
    "\n",
    "# LOAD ALL AS CATEGORIES\n",
    "dtypes = {}\n",
    "for x in FE+OHE: \n",
    "    dtypes[x] = 'category'\n",
    "dtypes['MachineIdentifier'] = 'str'\n",
    "dtypes['HasDetections'] = 'int8'\n",
    "\n",
    "# LOAD CSV FILE\n",
    "df_train = pd.read_csv(\"data/train.csv\", usecols=dtypes.keys(), dtype=dtypes, nrows = 2000000)\n",
    "print ('Loaded',len(df_train),'rows of TRAIN.CSV!')\n",
    "\n",
    "# DOWNSAMPLE\n",
    "sm = 1500000\n",
    "df_train = df_train.sample(sm)\n",
    "print ('Only using',len(df_train),'rows to train and validate')\n",
    "x=gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "1fadf38ea607fb94432c728a140c53f0f66139ca",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CHECK FOR NAN\n",
    "def nan_check(x):\n",
    "    if isinstance(x,float):\n",
    "        if math.isnan(x):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# FREQUENCY ENCODING\n",
    "def encode_FE(df,col,verbose=1):\n",
    "    d = df[col].value_counts(dropna=False)\n",
    "    n = col+\"_FE\"\n",
    "    df[n] = df[col].map(d)/d.max()\n",
    "    if verbose==1:\n",
    "        print('FE encoded',col)\n",
    "    return [n]\n",
    "\n",
    "# ONE-HOT-ENCODE ALL CATEGORY VALUES THAT COMPRISE MORE THAN\n",
    "# \"FILTER\" PERCENT OF TOTAL DATA AND HAS SIGNIFICANCE GREATER THAN \"ZVALUE\"\n",
    "def encode_OHE(df, col, filter, zvalue, tar='HasDetections', m=0.5, verbose=1):\n",
    "    cv = df[col].value_counts(dropna=False)\n",
    "    cvd = cv.to_dict()\n",
    "    vals = len(cv)\n",
    "    th = filter * len(df)\n",
    "    sd = zvalue * 0.5/ math.sqrt(th)\n",
    "    #print(sd)\n",
    "    n = []; ct = 0; d = {}\n",
    "    for x in cv.index:\n",
    "        try:\n",
    "            if cv[x]<th: break\n",
    "            sd = zvalue * 0.5/ math.sqrt(cv[x])\n",
    "        except:\n",
    "            if cvd[x]<th: break\n",
    "            sd = zvalue * 0.5/ math.sqrt(cvd[x])\n",
    "        if nan_check(x): r = df[df[col].isna()][tar].mean()\n",
    "        else: r = df[df[col]==x][tar].mean()\n",
    "        if abs(r-m)>sd:\n",
    "            nm = col+'_BE_'+str(x)\n",
    "            if nan_check(x): df[nm] = (df[col].isna()).astype('int8')\n",
    "            else: df[nm] = (df[col]==x).astype('int8')\n",
    "            n.append(nm)\n",
    "            d[x] = 1\n",
    "        ct += 1\n",
    "        if (ct+1)>=vals: break\n",
    "    if verbose==1:\n",
    "        print('OHE encoded',col,'- Created',len(d),'booleans')\n",
    "    return [n,d]\n",
    "\n",
    "# ONE-HOT-ENCODING from dictionary\n",
    "def encode_OHE_test(df,col,dt):\n",
    "    n = []\n",
    "    for x in dt: \n",
    "        n += encode_BE(df,col,x)\n",
    "    return n\n",
    "\n",
    "# BOOLEAN ENCODING\n",
    "def encode_BE(df,col,val):\n",
    "    n = col+\"_BE_\"+str(val)\n",
    "    if nan_check(val):\n",
    "        df[n] = df[col].isna()\n",
    "    else:\n",
    "        df[n] = df[col]==val\n",
    "    df[n] = df[n].astype('int8')\n",
    "    return [n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "afa1ab97768b80133161a00f0037f652f29e648b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FE encoded EngineVersion\n",
      "FE encoded AppVersion\n",
      "FE encoded AvSigVersion\n",
      "FE encoded Census_OSVersion\n",
      "OHE encoded RtpStateBitfield - Created 2 booleans\n",
      "OHE encoded IsSxsPassiveMode - Created 0 booleans\n",
      "OHE encoded DefaultBrowsersIdentifier - Created 1 booleans\n",
      "OHE encoded AVProductStatesIdentifier - Created 8 booleans\n",
      "OHE encoded AVProductsInstalled - Created 3 booleans\n",
      "OHE encoded AVProductsEnabled - Created 2 booleans\n",
      "OHE encoded CountryIdentifier - Created 39 booleans\n",
      "OHE encoded CityIdentifier - Created 11 booleans\n",
      "OHE encoded GeoNameIdentifier - Created 29 booleans\n",
      "OHE encoded LocaleEnglishNameIdentifier - Created 24 booleans\n",
      "OHE encoded Processor - Created 2 booleans\n",
      "OHE encoded OsBuild - Created 6 booleans\n",
      "OHE encoded OsSuite - Created 2 booleans\n",
      "OHE encoded SmartScreen - Created 4 booleans\n",
      "OHE encoded Census_MDC2FormFactor - Created 5 booleans\n",
      "OHE encoded Census_OEMNameIdentifier - Created 20 booleans\n",
      "OHE encoded Census_ProcessorCoreCount - Created 6 booleans\n",
      "OHE encoded Census_ProcessorModelIdentifier - Created 24 booleans\n",
      "OHE encoded Census_PrimaryDiskTotalCapacity - Created 14 booleans\n",
      "OHE encoded Census_PrimaryDiskTypeName - Created 3 booleans\n",
      "OHE encoded Census_HasOpticalDiskDrive - Created 1 booleans\n",
      "OHE encoded Census_TotalPhysicalRAM - Created 9 booleans\n",
      "OHE encoded Census_ChassisTypeName - Created 8 booleans\n",
      "OHE encoded Census_InternalPrimaryDiagonalDisplaySizeInInches - Created 20 booleans\n",
      "OHE encoded Census_InternalPrimaryDisplayResolutionHorizontal - Created 3 booleans\n",
      "OHE encoded Census_InternalPrimaryDisplayResolutionVertical - Created 5 booleans\n",
      "OHE encoded Census_PowerPlatformRoleName - Created 3 booleans\n",
      "OHE encoded Census_InternalBatteryType - Created 4 booleans\n",
      "OHE encoded Census_InternalBatteryNumberOfCharges - Created 4 booleans\n",
      "OHE encoded Census_OSEdition - Created 4 booleans\n",
      "OHE encoded Census_OSInstallLanguageIdentifier - Created 17 booleans\n",
      "OHE encoded Census_GenuineStateName - Created 1 booleans\n",
      "OHE encoded Census_ActivationChannel - Created 4 booleans\n",
      "OHE encoded Census_FirmwareManufacturerIdentifier - Created 13 booleans\n",
      "OHE encoded Census_IsTouchEnabled - Created 1 booleans\n",
      "OHE encoded Census_IsPenCapable - Created 0 booleans\n",
      "OHE encoded Census_IsAlwaysOnAlwaysConnectedCapable - Created 2 booleans\n",
      "OHE encoded Wdft_IsGamer - Created 2 booleans\n",
      "OHE encoded Wdft_RegionIdentifier - Created 12 booleans\n",
      "Encoded 322 new variables\n",
      "Removed original 43 variables\n"
     ]
    }
   ],
   "source": [
    "cols = []; dd = []\n",
    "\n",
    "# ENCODE NEW\n",
    "for x in FE:\n",
    "    cols += encode_FE(df_train,x)\n",
    "for x in OHE:\n",
    "    tmp = encode_OHE(df_train,x,0.005,5)\n",
    "    cols += tmp[0]; dd.append(tmp[1])\n",
    "print('Encoded',len(cols),'new variables')\n",
    "\n",
    "# REMOVE OLD\n",
    "for x in FE+OHE:\n",
    "    del df_train[x]\n",
    "print('Removed original',len(FE+OHE),'variables')\n",
    "x = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c99423ce7d2c30395ddddaf39ec7d56d87defa17"
   },
   "source": [
    "# Build and Train Network\n",
    "We will a build a 3 layer fully connected network with 100 neurons on each hidden layer. We will use ReLU activation, Batch Normalization, 40% Dropout, Adam Optimizer, and Decaying Learning Rate. Unfortunately we don't have an AUC loss function, so we will use Cross Entrophy instead. After each epoch, we will call a custom Keras callback to display the current AUC and continually save the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class printAUC(callbacks.Callback):\n",
    "    def __init__(self, X_train, y_train):\n",
    "        super(printAUC, self).__init__()\n",
    "        self.bestAUC = 0\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        pred = self.model.predict(np.array(self.X_train))\n",
    "        auc = roc_auc_score(self.y_train, pred)\n",
    "        print(\"Train AUC: \" + str(auc))\n",
    "        pred = self.model.predict(self.validation_data[0])\n",
    "        auc = roc_auc_score(self.validation_data[1], pred)\n",
    "        print (\"Validation AUC: \" + str(auc))\n",
    "        if (self.bestAUC < auc) :\n",
    "            self.bestAUC = auc\n",
    "            self.model.save(\"bestNet_new4.h5\", overwrite=True)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#SPLIT TRAIN AND VALIDATION SET\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    df_train[cols], df_train['HasDetections'], test_size = 0.5)\n",
    "\n",
    "# BUILD MODEL\n",
    "model = Sequential()\n",
    "model.add(Dense(100,input_dim=len(cols)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(100))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=Adam(lr=0.01), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "annealer = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 750000 samples, validate on 750000 samples\n",
      "Epoch 1/20\n",
      " - 101s - loss: 0.6452 - acc: 0.6196 - val_loss: 0.6314 - val_acc: 0.6333\n",
      "Train AUC: 0.6907954863596909\n",
      "Validation AUC: 0.690669993725627\n",
      "Epoch 2/20\n",
      " - 98s - loss: 0.6401 - acc: 0.6258 - val_loss: 0.6296 - val_acc: 0.6362\n",
      "Train AUC: 0.6950181322003872\n",
      "Validation AUC: 0.6941403838404253\n",
      "Epoch 3/20\n",
      " - 94s - loss: 0.6379 - acc: 0.6291 - val_loss: 0.6270 - val_acc: 0.6377\n",
      "Train AUC: 0.6984186986615704\n",
      "Validation AUC: 0.6967157123692704\n",
      "Epoch 4/20\n",
      " - 94s - loss: 0.6367 - acc: 0.6300 - val_loss: 0.6299 - val_acc: 0.6375\n",
      "Train AUC: 0.6986937156460172\n",
      "Validation AUC: 0.6966528099265408\n",
      "Epoch 5/20\n",
      " - 92s - loss: 0.6356 - acc: 0.6317 - val_loss: 0.6260 - val_acc: 0.6392\n",
      "Train AUC: 0.7008783299474548\n",
      "Validation AUC: 0.6980826907524236\n",
      "Epoch 6/20\n",
      " - 92s - loss: 0.6347 - acc: 0.6316 - val_loss: 0.6260 - val_acc: 0.6391\n",
      "Train AUC: 0.7021162055526275\n",
      "Validation AUC: 0.6987200714091184\n",
      "Epoch 7/20\n",
      " - 100s - loss: 0.6341 - acc: 0.6323 - val_loss: 0.6260 - val_acc: 0.6398\n",
      "Train AUC: 0.7021794349178196\n",
      "Validation AUC: 0.6988167547381369\n",
      "Epoch 8/20\n",
      " - 100s - loss: 0.6335 - acc: 0.6339 - val_loss: 0.6268 - val_acc: 0.6400\n",
      "Train AUC: 0.7036676272544389\n",
      "Validation AUC: 0.6992707694813587\n",
      "Epoch 9/20\n",
      " - 93s - loss: 0.6331 - acc: 0.6332 - val_loss: 0.6253 - val_acc: 0.6408\n",
      "Train AUC: 0.7047381003286343\n",
      "Validation AUC: 0.700059189173367\n",
      "Epoch 10/20\n",
      " - 94s - loss: 0.6324 - acc: 0.6347 - val_loss: 0.6263 - val_acc: 0.6405\n",
      "Train AUC: 0.704745924358381\n",
      "Validation AUC: 0.7000070788082782\n",
      "Epoch 11/20\n",
      " - 94s - loss: 0.6320 - acc: 0.6351 - val_loss: 0.6255 - val_acc: 0.6407\n",
      "Train AUC: 0.7062948255904395\n",
      "Validation AUC: 0.7009900793295465\n",
      "Epoch 12/20\n",
      " - 93s - loss: 0.6315 - acc: 0.6354 - val_loss: 0.6267 - val_acc: 0.6409\n",
      "Train AUC: 0.7065814199137028\n",
      "Validation AUC: 0.7009756694778383\n",
      "Epoch 13/20\n",
      " - 92s - loss: 0.6311 - acc: 0.6364 - val_loss: 0.6251 - val_acc: 0.6413\n",
      "Train AUC: 0.7068769709815682\n",
      "Validation AUC: 0.7010007152397754\n",
      "Epoch 14/20\n",
      " - 94s - loss: 0.6309 - acc: 0.6364 - val_loss: 0.6240 - val_acc: 0.6414\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,Y_train, batch_size=32, epochs = 20, callbacks=[annealer,printAUC(X_train, Y_train)], validation_data = (X_val,Y_val), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(history.history.keys())\n",
    "#  \"Accuracy\"\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Loss\"\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = {}\n",
    "h = dtypes\n",
    "h.pop('HasDetections', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "96e7fbf730e30707f04e4b192c839c820af5dee4"
   },
   "outputs": [],
   "source": [
    "#del df_train\n",
    "#del X_train, X_val, Y_train, Y_val\n",
    "x = gc.collect()\n",
    "\n",
    "# LOAD BEST SAVED NET\n",
    "\n",
    "model = load_model('bestNet_new3.h5')\n",
    "\n",
    "pred = np.zeros((7853253,1))\n",
    "id = 1\n",
    "chunksize = 200000\n",
    "for df_test in pd.read_csv('data/test.csv',chunksize = chunksize, usecols=h, dtype=h, nrows = 200000):\n",
    "    print ('Loaded',len(df_test),'rows of TEST.CSV!')\n",
    "    # ENCODE TEST\n",
    "    cols = []\n",
    "    for x in FE:\n",
    "        cols += encode_FE(df_test,x,verbose=0)\n",
    "    for x in range(len(OHE)):\n",
    "        cols += encode_OHE_test(df_test,OHE[x],dd[x])\n",
    "    # PREDICT TEST\n",
    "    end = (id)*chunksize\n",
    "    if end>7853253: end = 7853253\n",
    "    pred[(id-1)*chunksize:end] = model.predict(df_test[cols])\n",
    "    print('  encoded and predicted part',id)\n",
    "    id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SUBMIT TO KAGGLE\n",
    "df_test = pd.read_csv('data/test.csv', usecols=['MachineIdentifier'])\n",
    "df_test['HasDetections'] = pred\n",
    "df_test.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del df_train\n",
    "#del X_train, X_val, Y_train, Y_val\n",
    "x = gc.collect()\n",
    "\n",
    "# LOAD BEST SAVED NET\n",
    "\n",
    "model = load_model('bestNet_new111.h5')\n",
    "\n",
    "pred = np.zeros((200000,1))\n",
    "id = 1\n",
    "chunksize = 200000\n",
    "for df_test in pd.read_csv('data/train.csv',chunksize = chunksize, usecols=h, dtype=h, skiprows = range(1,2000000),\n",
    "                           nrows = 200000):\n",
    "    print ('Loaded',len(df_test),'rows of TEST.CSV!')\n",
    "    # ENCODE TEST\n",
    "    cols = []\n",
    "    #pred_label = df_test['HasDetections']\n",
    "    #del df_test['HasDetections']\n",
    "    for x in FE:\n",
    "        cols += encode_FE(df_test,x,verbose=0)\n",
    "    for x in range(len(OHE)):\n",
    "        cols += encode_OHE_test(df_test,OHE[x],dd[x])\n",
    "    # PREDICT TEST\n",
    "    end = (id)*chunksize\n",
    "    if end>7853253: end = 7853253\n",
    "    pred[(id-1)*chunksize:end] = model.predict(df_test[cols])\n",
    "    print('  encoded and predicted part',id)\n",
    "    id += 1\n",
    "    \n",
    "#df_test = pd.read_csv(\"data/train.csv\", usecols=dtypes.keys(), dtype=dtypes, ,\n",
    "#                      nrows = 200000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('data/train.csv', usecols=['MachineIdentifier'], skiprows = range(1,2000000),\n",
    "                           nrows = 200000)\n",
    "df_test['HasDetections'] = pred\n",
    "pred_label = pd.read_csv('data/train.csv', usecols = ['HasDetections'], skiprows = range(1,2000000), nrows = 200000)\n",
    "\n",
    "pred_label.head()\n",
    "#df_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = list(pred)\n",
    "trueres = list(pred_label['HasDetections'])\n",
    "\n",
    "print(len(trueres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truecount = 0\n",
    "falsecount = 0\n",
    "d = -1\n",
    "for i in range(0, len(result)):\n",
    "    if result[i] > 0.5:\n",
    "        d = 1\n",
    "    else:\n",
    "        d = 0\n",
    "    if d == trueres[i]:\n",
    "        truecount+=1\n",
    "    else:\n",
    "        falsecount+=1\n",
    "        \n",
    "print(truecount)\n",
    "print(truecount/200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
